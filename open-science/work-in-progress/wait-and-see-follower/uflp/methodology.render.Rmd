# UFLP

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(ggplot2)
library(rmarkdown)
suppressWarnings(library(dplyr))
library(knitr)
library(kableExtra)
library(stringr)
library(RColorBrewer)
library(tidyr)
```

## Some helper functions first

```{r}
make_heatmap = function(data) {
  
  max_count = data %>%
    count(n_facilities, n_customers) %>%
    pull(n) %>%
    max()
  
  # Step 1: Get all existing combinations in the original data
  existing_combinations = data %>% distinct(n_facilities, n_customers)
  
  # Step 2: Get all combinations of existing values
  full_grid = expand_grid(
    n_facilities = unique(data$n_facilities),
    n_customers = unique(data$n_customers)
  )
  
  # Step 3: Mark whether the pair exists in the original data
  full_grid = full_grid %>%
    left_join(existing_combinations %>% mutate(exists = TRUE), by = c("n_facilities", "n_customers"))
  
  # Step 4: Count how many rows are below the time limit
  counts = data %>%
    filter(total_time < 10800) %>%
    count(n_facilities, n_customers, name = "count")
  
  # Step 5: Join the count info into the full grid
  summary_data = full_grid %>%
    left_join(counts, by = c("n_facilities", "n_customers"))
  
  # Step 6: Set count = 0 where pair exists but no row meets time condition
  summary_data = summary_data %>%
    mutate(
      count = ifelse(is.na(count) & exists == TRUE, 0, count)
    )
  
  # Step 7: Plot with text
  p = ggplot(summary_data, aes(x = factor(n_facilities), y = factor(n_customers), fill = count)) +
    geom_tile(color = "white") +
    geom_text(aes(label = ifelse(!is.na(count), count, "")), color = "black") +
    scale_fill_gradient(
      low = "white", high = "darkgreen", na.value = "black",
      limits = c(0,max_count)
    ) +
    labs(
      title = "Number of solved instances within the time limit",
      x = "Number of Facilities",
      y = "Number of Customers",
      fill = "Count"
    ) +
    theme_minimal()
  
  return(p)
}

```

```{r}
make_ecdf_total_time = function(data) {
  p = ggplot(data, aes(x = total_time, color = factor(Gamma), linetype = full_approach ) ) +
      stat_ecdf(geom = "step", linewidth = .5) +
      #scale_y_continuous(labels = function(x) gsub("%", "\\\\%", scales::label_percent()(x))) +
      labs(
        title = "ECDF of Total Computation Time",
        x = "Time (s)",
        y = "Instances",
        color = "Uncertainty budget",
        linetype = "Solution Approach"
      ) +
      xlim(c(0,10800)) +
      theme_minimal() +
      scale_color_brewer(palette = "Set1")
  
  print(p)
  
  ecdf_table <- all_results_wait_and_see %>%
    group_by(Gamma, full_approach) %>%
    arrange(total_time) %>%
    mutate(y = ecdf(total_time)(total_time)) %>%
    select(Gamma, full_approach, x = total_time, y)
  
  return(ecdf_table)
}

make_ecdf_gap = function(data) {
  
  p = ggplot(data,
         aes(x = (best_obj - best_bound) / (abs(best_obj) + 1e-10), color = factor(Gamma), linetype = full_approach ) ) +
      stat_ecdf(geom = "step", linewidth = .5) +
      scale_x_continuous(labels = scales::percent) +
      labs(
        title = "ECDF of relative gap with optimality separation",
        x = "Gap (%)",
        y = "Instances (%)",
        color = "Uncertainty budget",
        linetype = "Solution Approach"
      ) +
      theme_minimal() +
      scale_color_brewer(palette = "Set1")
  
  return (p)
}

make_ecdf_master_time = function(data) {
  
  p = ggplot(data,
         aes(x = master_time / total_time, color = factor(Gamma), linetype =  full_approach) ) +
      stat_ecdf(geom = "step", linewidth = .5) +
      scale_x_continuous(labels = scales::percent) +
      scale_y_continuous(labels = scales::percent) +
      labs(
        title = "ECDF of master time / separation time",
        x = "Master time /  Total time (%)",
        y = "Instances (%)",
        color = "Uncertainty budget",
        linetype = "Solution Approach"
      ) +
      theme_minimal() +
      scale_color_brewer(palette = "Set1")
    
  return (p)
}

make_master_time_barplot = function(data) {
  
  # Compute percentages
  plot_data <- data %>%
    dplyr::mutate(
      master_pct = master_time / total_time,
      other_pct  = 1 - master_pct
    ) %>%
    tidyr::pivot_longer(
      cols = c(master_pct, other_pct),
      names_to = "component",
      values_to = "percentage"
    )
  
  # Bar plot
  p <- ggplot(plot_data,
              aes(x = factor(Gamma), y = percentage, fill = component)) +
    geom_bar(stat = "summary", fun = mean, position = "stack") +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_manual(
      values = c(master_pct = "#E41A1C", other_pct = "#377EB8"),
      labels = c("Solving the Master Problem", "Solving the Separation Problem"),
      name = "Time Component"
    ) +
    facet_wrap(~ full_approach) +
    labs(
      title = "Average percentage of time spent on master vs separation",
      x = "Uncertainty budget (Gamma)",
      y = "Average Time (%)"
    ) +
    theme_minimal()
  
  return(p)
}

```

## Robust bilevel problem with wait-and-see follower

```{r}
all_results_wait_and_see = rbind(
  read.csv("ws_results.csv", header = FALSE),
  read.csv("aro_results.csv", header = FALSE)
)
colnames(all_results_wait_and_see) = c("tag",
                          "instance",
                          "lower_objective_instance",
                          "n_facilities",
                          "n_customers", 
                          "d",
                          "angle",
                          "Gamma",
                          "master_method",
                          "separation_method",
                          "status",
                          "reason",
                          "best_bound",
                          "best_obj",
                          "total_time",
                          "master_time",
                          "separation_time",
                          "n_iterations")

all_results_wait_and_see = all_results_wait_and_see %>%
  mutate(
    separation_type = sub("_.*", "", separation_method),
    separation_method = gsub("_", " ", separation_method),
    separation_approach = sub("^[^_]+_", "", separation_method),
    full_approach = paste0(master_method, " - ", separation_approach),
    sigma_square = as.numeric(sub(".*_\\d+_(\\d+)_\\d+\\.txt$", "\\1", lower_objective_instance)),
    total_time = pmin(total_time, 10800),
    gap = (best_obj - best_bound) / (abs(best_obj) + 1e-10)
  )

#all_results_wait_and_see = all_results_wait_and_see %>% filter(n_facilities <= 25 & n_customers <= 40)

all_results_wait_and_see$tag = NULL

#print("*********************** WARNING: Filtering instances here ***********************")

#all_results_wait_and_see <- all_results_wait_and_see %>%
#  group_by(instance, lower_objective_instance, Gamma) %>%
#  mutate(n_full_approach = n_distinct(full_approach)) %>%
#  ungroup() %>%
#  filter(n_full_approach == n_distinct(all_results_wait_and_see$full_approach)) %>%
#  select(-n_full_approach)

paged_table(all_results_wait_and_see)
```

### How Does the Algorithm Perform?

```{r}

table = make_ecdf_total_time(all_results_wait_and_see)

write_ecdf_csv = function(data, column_name, filename) {
  
  x_points <- seq(0, max(data[[column_name]]), length.out = 100)

  combinations <- data %>%
    select(full_approach, Gamma) %>%
    distinct()
  
  # Compute ECDF values for each combination
  ecdf_table <- combinations %>%
    rowwise() %>%
    do({
      approach <- .$full_approach
      gamma <- .$Gamma
      
      # Subset the data
      df_sub <- data %>%
        filter(full_approach == approach, Gamma == gamma)
      
      # Compute ECDF
      F <- ecdf(df_sub[[column_name]])
      
      # Evaluate at x_points
      data.frame(
        x = x_points,
        value = F(x_points),
        full_approach = approach,
        Gamma = gamma
      )
    }) %>%
    ungroup()
  
  ecdf_wide <- ecdf_table %>%
    unite("approach_Gamma", full_approach, Gamma, sep = "_") %>%
    pivot_wider(names_from = approach_Gamma, values_from = value)
  
  
  ecdf_wide
  
  write.csv(ecdf_wide, filename, row.names = FALSE, quote = FALSE)
}

write_ecdf_csv(all_results_wait_and_see, "total_time", "rap-ecdf-time.csv")
write_ecdf_csv(all_results_wait_and_see, "gap", "rap-ecdf-gap.csv")

make_ecdf_gap(all_results_wait_and_see)
make_ecdf_master_time(all_results_wait_and_see)
make_master_time_barplot(all_results_wait_and_see)
```

## What Can We Solve? 

```{r}
wait_and_see_big_M = all_results_wait_and_see %>% filter(full_approach == "BigM - Optimality KKT BigM")
make_heatmap(wait_and_see_big_M)
```

## How Many Iterations?

```{r}

wait_and_see_big_M_solved = wait_and_see_big_M %>% filter(total_time < 10800)

avg_iter_wait_and_see <- wait_and_see_big_M_solved %>%
  group_by(n_facilities, n_customers) %>%
  summarise(avg_iterations = mean(n_iterations, na.rm = TRUE), .groups = "drop") %>%
  mutate(problem_type = "Wait-and-see")

# Step 3: Create a label for each pair to use on the x-axis
avg_iterations_combined <- avg_iter_wait_and_see %>%
  mutate(pair = paste0("(", n_facilities, ", ", n_customers, ")"))

# Step 4: Plot side-by-side bars
ggplot(avg_iterations_combined, aes(x = pair, y = avg_iterations, fill = problem_type)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
  labs(
    title = "Average number of iterations per instance size",
    x = "Instance size",
    y = "Average number of iterations",
    fill = "Problem type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Summary Table

```{r}
summary_table <- all_results_wait_and_see %>%
  filter(total_time < 10800) %>%
  group_by(full_approach, Gamma, n_facilities, n_customers) %>%
  summarise(
    #instance = paste(unique(instance), collapse = ", "),
    n_instances = n(),
    avg_total_time = mean(total_time, na.rm = TRUE),
    avg_master_share = mean(master_time / total_time * 100, na.rm = TRUE),
    avg_n_iterations = mean(n_iterations, na.rm = TRUE),
    .groups = "drop"
  )

kable(summary_table)
```

## Is the Angle Making Things Harder?

```{r}
wait_and_see_big_M %>% group_by(sigma_square) %>% summarise(avg_angle = mean(angle))

ggplot(wait_and_see_big_M, aes(x = factor(sigma_square), y = angle)) +
  geom_boxplot(outlier.alpha = 0.3, fill = "lightblue") +
  labs(
    title = "Distribution of Angle by Sigma Square",
    x = "Sigma Square",
    y = "Angle"
  ) +
  theme_minimal()

######################

for (Gamma_val in unique(wait_and_see_big_M$Gamma)) {
  
  data = wait_and_see_big_M %>% filter(Gamma == Gamma_val, n_facilities <= 15)
  
  # Count long runs per exact angle
  counts_per_angle <- data %>%
    group_by(angle) %>%
    summarise(long_runs = sum(total_time >= 10800, na.rm = TRUE)) %>%
    ungroup()
  
  # Plot with points, smooth line, and count labels
  p = ggplot(data, aes(x = angle, y = total_time)) +
    geom_point(alpha = 0.4, size = 2) +
    geom_smooth(method = "loess", se = FALSE, color = "red", size = 1) +
    geom_text(
      data = counts_per_angle %>% filter(long_runs > 0),
      aes(x = angle, y = max(data$total_time, na.rm = TRUE) * 1.05, label = long_runs),
      color = "blue",
      size = 3,
      vjust = 0
    ) +
    labs(
      title = paste0("Total Time vs Angle (Gamma = ", Gamma_val, " and n_facilites <= 15)"),
      x = "Angle",
      y = "Total Time (seconds)",
      caption = "Blue numbers: count of runs with total_time >= 10800 at each angle"
    ) +
    theme_minimal() +
    coord_cartesian(clip = "off")  # to show labels outside plot area
  
  print(p)
}
```


## Sanity Check

```{r}
check_obj_consistency <- function(data) {
  data %>%
    filter(total_time < 10800) %>%
    group_by(instance, lower_objective_instance, Gamma) %>%
    summarise(
      n_methods = n_distinct(full_approach),
      n_obj     = n_distinct(best_obj),
      same_obj  = (n_obj == 1),
      .groups = "drop"
    ) %>%
    filter(!same_obj)
}

check_obj_consistency(all_results_wait_and_see)

all_results_wait_and_see %>% filter(n_iterations > 100)
```
