---
title: Open methodology for "Adjustable robust optimization with discrete uncertainty" > DisruptionFLP
output: 
  html_document:
    theme: null
    css: /assets/css/design.css
    self_contained: false
    highlight: null
    include:
      in_header: ../../../_includes/head.html
      before_body: 
        - ../../../_includes/header.html
        - ../../../_includes/begin_content.html
        - ../../../_includes/toc.html
        - ../../../_includes/begin_post_content.html
      after_body: 
        - ../../../_includes/handle_page_title.html
        - ../../../_includes/end_post_content.html
        - ../../../_includes/end_content.html
        - ../../../_includes/footer.html
---

<div class="warning">This document is automatically generated after every `git push` action on the public repository `hlefebvr/hlefebvr.github.io` using rmarkdown and Github Actions. This ensures total reproducibility of our data manipulation. Last automatic compilation: `r format(Sys.time(), '%d/%m/%y %H:%M:%S')`.</div>

```{r echo=FALSE}
library(rmarkdown)
```


## Reading results

```{r echo = FALSE}
library(rmarkdown)
library(kableExtra)

```

### CSV structures

#### Benders approaches

The results obtained by the Benders-type approaches are given in CSV form with the following columns.

- *instance*: the instance filename ;
- *gamma*: the value of $\Gamma$ ; 
- *status*: the status of the solution (Optimal, Feasible or Infeasible) ;
- *reason*: the reason for the solution status (Proved or TimeLimit) ;
- *objective*: the objective value reported ;
- *time*: the computation time spent solving the problem ;
- *nodes*: the overall number of created nodes.

There are 4 different files, each named `results_X_Y.csv` where 
- X is 1 if $x$ is binary and 0 if it is relaxed in the separation problem ;
- Y is 1 if the IIS technique is used when branching and 0 otherwise.

For instance, `results_1_0.csv` contains the following.
```{r echo = FALSE}
raw_benders = read.csv("./results_1_0.csv", header = FALSE)
colnames(raw_benders) <- c("instance", "gamma", "status", "reason", "objective", "time", "nodes")
paged_table(raw_benders)
```

### Unifying format

To make our study easier, we start by unifying the format of each dataset. To do so, we transform our data to obtain it in the following format:

- *instance*: the instance filename ;
- *n_facilities*: the number of facilities ;
- *n_customers*: the number of customers ;
- *gamma*: the value of $\Gamma$ ; 
- *objective*: the best objective value found (feasible) ;
- *time*: the computation time spent solving the instance.

We thus introduce two functions `read_csv_benders` and `read_csv_ccg` which first reads an input file for the corresponding approach and returns the associated formatted table.

Before that, we first introduce a helper function `parse_instance_properties` which takes a list of instances as input and returns a table containing, for each instance, the number of knapsacks, the number of items and the value for alpha extracted from the instance file name.

```{r}
library(stringr)

parse_instance_properties = function (instances) {
  
  parsed = t(apply(as.matrix(instances), 1, function(str)  str_extract_all(str, regex("([0-9]+)"))[[1]]))
  
  result = data.frame(instances, as.double(parsed[,1]), as.double(parsed[,2]))
  colnames(result) = c("instance", "n_facilities", "n_customers")
  
  return (result)
  
}
```

#### read_csv_benders

The `read_csv_benders`` function is given as follows.

```{r}
read_csv_benders = function(filename) {
  
  # Read raw results
  raw_results = read.csv(filename, header = FALSE)
  colnames(raw_results) <- c("instance", "gamma", "status", "reason", "objective", "time", "nodes")
  
  # Fix unsolved instances to TIME_LIMIT
  if (sum(raw_results$time >= 3600) > 0) {
    raw_results[raw_results$time >= 3600,]$time = 3600
  }
  
  # Extract properties from instance file names
  properties = parse_instance_properties(raw_results$instance)
  
  # Build result data frame
  result = data.frame(
      properties$instance, 
      properties$n_facilities,
      properties$n_customers,
      raw_results$gamma,
      raw_results$objective,
      raw_results$time,
      raw_results$nodes
    )
  colnames(result) = c("instance", "n_facilities", "n_customers", "gamma", "objective", "time", "nodes")
  
  return (result)
  
}
```

We can then read CSV files coming from the benders approach.

```{r}
benders_con_gen = read_csv_benders("./results_0_0.csv")
benders_bin_gen = read_csv_benders("./results_1_0.csv")
benders_con_iis = read_csv_benders("./results_0_1.csv")
benders_bin_iis = read_csv_benders("./results_1_1.csv")
```


#### read_csv_ccg

The `read_csv_ccg`` function is given as follows.

```{r}
read_csv_ccg = function (filename) {
  
  # Read raw results
  raw_results = read.csv(filename, header = FALSE)
  colnames(raw_results) <- c("instance", "gamma", "iter", "LB", "UB", "time", "inner_iter_1", "inner_iter_2")
  
  # Fix unsolved instances to TIME_LIMIT
  if (sum(raw_results$time >= 3600) > 0) {
    raw_results[raw_results$time >= 3600,]$time <- 3600
  }
  
  # Extract properties from instance file names
  properties = parse_instance_properties(raw_results$instance)
  
  # Build result data frame
  result = data.frame(
      properties$instance, 
      properties$n_facilities,
      properties$n_customers,
      raw_results$gamma,
      raw_results$UB,
      raw_results$time
    )
  colnames(result) = c("instance", "n_facilities", "n_customers", "gamma", "objective", "time")
  
  return (result)
  
}
```

Then, we read the results obtained by the CCG approach as follows.

```{r}
ccg = read_csv_ccg("./results_ccg_DisruptionFLP.csv")
```


### Checking

```{r}
compare = function(a, b) {
  A = a[a$time < 3600,c("instance", "gamma", "objective", "time")]
  B = b[b$time < 3600,c("instance", "gamma", "objective", "time")]
  
  merged = merge(A, B, by = c("instance", "gamma"))
  
  filter = abs(merged$objective.x - merged$objective.y) > 1e-3
  if (sum( filter ) > 0) {
    paged_table( merged[filter,] )
  }
}

compare(benders_con_gen, benders_bin_gen)
compare(benders_con_gen, benders_con_iis)
compare(benders_con_gen, benders_bin_iis)
```

## Analysis

### Computational times

Our first analysis regards computational times within different groups. We will group data by `group_by` columns. Note that `str_group_by` contains a "stringified" version of the columns names for clean outputs.

```{r}
group_by = c("gamma", "n_facilities", "n_customers")
str_group_by = c("$\\Gamma$", "$|V_1|$", "$|V_2|$")
```

#### Computing summary times by group

We start by calling `summary` on each group. This will compute, for each group, the minimum, 1st quantile, median, mean, 3rd quantile and maximum execution time. This is done in the following function which takes as parameter the formatted table with all results.

```{r}
compute_summary_by_group = function(data) {
  
  # We summarize only the solved instances
  data = data[data$time < 3600,]
  
  # We aggregate by `group_by` using `summary`
  result = aggregate(data$time, by = data[,group_by], summary)
  
  # Here, we call data.frame recursively to flatten `result` (summary does create a nested structure)
  result = do.call(data.frame, result)
  
  # Then, we set column names
  colnames(result) = c(group_by, "min", "1st_quantile", "median", "mean", "3rd_quantile", "max")
  
  return (result)
}
```

We then call this function on each approach.

```{r}
summary_ccg = compute_summary_by_group(ccg)
summary_benders_con_gen = compute_summary_by_group(benders_con_gen)
summary_benders_bin_gen = compute_summary_by_group(benders_bin_gen)
summary_benders_con_iis = compute_summary_by_group(benders_con_iis)
summary_benders_bin_iis = compute_summary_by_group(benders_bin_iis)
```

#### Computing number of unsolved instances by group

We then count the number of instances which could not be solved to optimality within each group. This is done in the following function.

```{r}
compute_unsolved_by_group = function(data) {

  # Aggregate groups using `sum` over the filter returning 1 iff the time limit is reached
  result = aggregate(data$time >= 3600, by = data[,group_by], sum)
  
  # Set column and row names
  colnames(result) = c(group_by, "unsolved")
  rownames(result) = NULL
  
  return (result)
  
}
```

Again, we call this function on each approach.

```{r}
unsolved_ccg = compute_unsolved_by_group(ccg)
unsolved_benders_con_gen = compute_unsolved_by_group(benders_con_gen)
unsolved_benders_bin_gen = compute_unsolved_by_group(benders_bin_gen)
unsolved_benders_con_iis = compute_unsolved_by_group(benders_con_iis)
unsolved_benders_bin_iis = compute_unsolved_by_group(benders_bin_iis)
```

#### Computing number of instances by group

Finally, we also count the number of instances which was tried by each approach. (Note that when experiments are done, they should all be equal).

```{r}
compute_instances_by_group = function(data) {
  
  # Call length on each group to count the number of instances
  result = aggregate(data$instance, by = data[,group_by], length)
  
  # Set row and column names
  colnames(result) = c(group_by, "total")
  rownames(result) = NULL
  
  return (result)
  
}
```

Let's call it on each approach.

```{r}
total_ccg = compute_instances_by_group(ccg)
total_benders_con_gen = compute_instances_by_group(benders_con_gen)
total_benders_bin_gen = compute_instances_by_group(benders_bin_gen)
total_benders_con_iis = compute_instances_by_group(benders_con_iis)
total_benders_bin_iis = compute_instances_by_group(benders_bin_iis)
```


#### Computing number of infeasible problems

Finally, we also count the number of instances which was tried by each approach. (Note that when experiments are done, they should all be equal).

```{r}
compute_infeasible_by_group = function(data) {
  
  # Call length on each group to count the number of instances
  result = aggregate(data$objective == 0, by = data[,group_by], sum)
  
  # Set row and column names
  colnames(result) = c(group_by, "infeasible")
  rownames(result) = NULL
  
  return (result)
  
}
```

Let's call it on each approach.

```{r}
infeasible_ccg = compute_infeasible_by_group(ccg)
infeasible_benders_con_gen = compute_infeasible_by_group(benders_con_gen)
infeasible_benders_bin_gen = compute_infeasible_by_group(benders_bin_gen)
infeasible_benders_con_iis = compute_infeasible_by_group(benders_con_iis)
infeasible_benders_bin_iis = compute_infeasible_by_group(benders_bin_iis)

paged_table(infeasible_ccg)
```


#### Final result

We are now ready to build our summary table. In what follows, we introduce a helper function to add the useful columns of a by-group result (i.e., `total_<APPROACH>`, `unsolved_<APPROACH>` or `summary_<APPROACH>`) to the main table, called `Table`.

```{r}
# Helper function
add_columns = function(table, data, suffix) {
  result = merge(table, data, by = group_by, all = TRUE, suffixes = c("", paste(".", suffix, sep = "")))
  return (result)
}

# We list the useful columns of the by-group results
column_names = c("total", "unsolved", "min", "1st_quantile", "median", "mean", "3rd_quantile", "max")

# We create an empty data frame with column names. This is used to
# (1) create the placeholder for the group identifiers
# (2) force R to rename each added column by appending a suffix to it
Table = data.frame(matrix(ncol = length(group_by) + length(column_names), nrow = 0))
colnames(Table) = c(group_by, column_names)

# We add by-group totals
Table = add_columns(Table, total_benders_con_gen, "con_gen")
Table = add_columns(Table, total_benders_bin_gen, "bin_gen")
Table = add_columns(Table, total_benders_con_iis, "con_iis")
Table = add_columns(Table, total_benders_bin_iis, "bin_iis")
Table = add_columns(Table, total_ccg, "ccg")

# We add by-group unsolveds
Table = add_columns(Table, unsolved_benders_con_gen, "con_gen")
Table = add_columns(Table, unsolved_benders_bin_gen, "bin_gen")
Table = add_columns(Table, unsolved_benders_con_iis, "con_iis")
Table = add_columns(Table, unsolved_benders_bin_iis, "bin_iis")
Table = add_columns(Table, unsolved_ccg, "ccg")

# We add by-group summaries
Table = add_columns(Table, summary_benders_con_gen, "con_gen")
Table = add_columns(Table, summary_benders_bin_gen, "bin_gen")
Table = add_columns(Table, summary_benders_con_iis, "con_iis")
Table = add_columns(Table, summary_benders_bin_iis, "bin_iis")
Table = add_columns(Table, summary_ccg, "ccg")

# Finally, we can remove the "fake" columns we introduced to foce R renaming new columns
Table = Table[,!names(Table) %in% column_names]
```

The resulting table therefore contains a merge of all by-group data and is drawn hereafter.

```{r echo = FALSE}
paged_table(Table)
```


To better summarize our data, we make a new table out of this table by selecting interesting columns. This is done as follows.

```{r}
Table1 = Table[,c(
  group_by,
  "total.bin_gen",
  "total.ccg",
  "unsolved.bin_gen",
  "unsolved.ccg",
  "mean.bin_iis",
  "mean.ccg"
  )]
```

```{r echo = FALSE}
digits = c(as.vector(matrix(0, nrow = length(group_by))), 0, 0, 0, 0, 2, 2)
knitr::kable(Table1,
             digits = digits,
             col.names = c(str_group_by, "Benders", "CCG", "Benders", "CCG", "Benders", "CCG")
             ) %>%
      kable_classic() %>%
      add_header_above(c(" " = length(group_by), "Total" = 2, "Unsolved" = 2, "Time" = 2))
```

```{r}
Table2 = Table[,c(
  group_by,
  "total.bin_gen",
  "total.bin_iis",
  "total.con_gen",
  "total.con_iis",
  "unsolved.bin_gen",
  "unsolved.bin_iis",
  "unsolved.con_gen",
  "unsolved.con_iis",
  "mean.bin_gen",
  "mean.bin_iis",
  "mean.con_gen",
  "mean.con_iis"
  )]
```

```{r echo = FALSE}
digits = c(as.vector(matrix(0, nrow = length(group_by))), 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2)
knitr::kable(Table2,
             digits = digits,
             col.names = c(str_group_by, "B-Gen", "B-IIS", "C-Gen", "C-IIS", "B-Gen", "B-IIS", "C-Gen", "C-IIS", "B-Gen", "B-IIS", "C-Gen", "C-IIS")
             ) %>%
      kable_classic() %>%
      add_header_above(c(" " = length(group_by), "Total" = 4, "Unsolved" = 4, "Time" = 4))
```


### Performance profiles

In this section, we plot performance profiles for the different approaches. 

#### Computations

We start by merging every approach results together so as to have one table in which each row, referring to a given instance, gathers all results from every approaches. We do this by introducing a helper function called `add_approach`. See rather.

```{r}
# Helper function
add_approach = function(table, approach, suffix) {
  
  # Merge results by instance name and value for gamma
  result = merge(approach[,c("instance", "gamma", "time")], table, by = c("instance", "gamma"))
  
  # Here, we rename column "time" as "time.<SUFFIX>"
  colnames(result)[3] = paste("time.", suffix, sep = "")
  rownames(result) = NULL
  
  return (result)
}

# We first create an initial table
all_approaches = data.frame(
  "instance" = benders_con_gen$instance, 
  "gamma" = benders_con_gen$gamma)

# We then add other approaches
all_approaches = add_approach(all_approaches, ccg, "ccg")
#all_approaches = add_approach(all_approaches, benders_con_gen, "con_gen")
#all_approaches = add_approach(all_approaches, benders_bin_gen, "bin_gen")
#all_approaches = add_approach(all_approaches, benders_con_iis, "con_iis")
all_approaches = add_approach(all_approaches, benders_bin_iis, "bin_iis")
```

For each instance, we compute the best time among all approaches.

```{r}
# Compute row-wise minimums
all_approaches$time.best = apply(all_approaches[,-c(1:2)], 1, FUN = min)
```

Then, we compute the performance ratios of each approach, this is defined as
$$
  \tau_i = \frac{ t(approach_i) }{ t(best\_approach_i) }
$$
where $i$ is an index for instances with its value for $\Gamma$.

To make our code clearer, we first introduce some variables.

```{r}
approach_columns = c(3:(ncol(all_approaches)-1))
approach_names = colnames(all_approaches[,approach_columns])
n_approaches = length(approach_names)
```

These variables contain respectively the indices for columns containing the approaches execution times, the approaches given names and the number of approaches considered.

We can now compute performance ratios as follows.

```{r}
# Compute performance ratios
performance_ratios = all_approaches[,approach_columns] / all_approaches$time.best

# Rename columns with ratio.time.<APPROACH>
colnames(performance_ratios) = paste0("ratio.", approach_names)

# Bind the performance ratios to the main result
all_approaches = cbind(all_approaches, performance_ratios)
```

When an approach times out, its performance ratio should be the maximum performance ratio of all. We do this in the following.

```{r}
# Compute the worst performance ratio
worst_performance_ratio = max(performance_ratios) + 1

# For each approach
for (approach in approach_names) {
  # Set the ratio of unsolved instances to the worst
  all_approaches[all_approaches[,approach] >= 3600, paste0("ratio.", approach)] = worst_performance_ratio
}
```

Finally, we end by adding parsed instance properties to our table. This will allow us to plot performance profiles according to different criteria.

```{r}
all_approaches = data.frame(
  parse_instance_properties(all_approaches$instance),
  gamma = all_approaches$gamma,
  all_approaches[,c(3:(ncol(all_approaches)))]
)
```

The resulting table is drawn here.

```{r echo = FALSE}
paged_table(all_approaches)
```

#### Final result

All the data has been prepared to be able to plot the performance profile of each instances. Note that performance profiles are the ECDF of ratios $\{ \tau_i \}$ for each approach.
We introduce function `plot_performance_profile` which plots the performance profile over a restricted data set.

```{r}
plot_performance_profile = function(data, xlim = c(1, 50), color = palette(), main = "Performance profile") {
  
  # We first select the ratio columns (note that this uses the "global variable" n_approaches)
  ratios = data[, c( ( ncol(data) - n_approaches + 1 ) : ncol(data)  ) ]
  
  # We create a two-column table where each line is [ ratio, group ] with:
  # - ratio being the ratio for a given instance
  # - group being the approach to which the ratio "belongs"
  ratios_by_approach = data.frame(
                          ratio = unlist(ratios),
                          group = gl( n_approaches, nrow(ratios), labels = approach_names )
                        )
  
  # We then compute the ECDF for each approach by calling aggregate with ecdf
  performance_profiles = aggregate(ratios_by_approach$ratio, by = list(ratios_by_approach$group), ecdf)
  colnames(performance_profiles) = c("approach", "performance_profile")
  
  # For each approach, we plot the result
  for (index in 1:n_approaches) {
    
    # If this is the first plot, we need to call "plot", otherwise, we call "lines"
    plot_function = lines
    if (index == 1) {
      plot_function = plot
    }
    
    # We retrieve the ECDF function
    ecdf_function = performance_profiles[index,]$performance_profile[[1]] 
    
    # Actually plot the function
    plot_function( ecdf_function, xlim = xlim, ylim = c(0, 1), lty = "solid", cex = 0, col = color[index], main = "", xlab = "", ylab = "" )
    
  }
  
  # Set the plot title
  title(main = main,
        xlab = "Performance ratio",
        ylab = "ECDF")
  
  # Set the plot legend
  legend(
    xlim[2] / 2,
    .5,
    legend = approach_names,
    lty = "solid",
    col = color
  )
  
}
```

Now, we can use this function to plot differnet performance profiles.

```{r, figures-side, fig.show="hold", out.width="100%"}
plot_performance_profile(all_approaches, main = "All instances")
plot_performance_profile(all_approaches[all_approaches$gamma == 2,], main = "Gamma = 2")
plot_performance_profile(all_approaches[all_approaches$gamma == 3,], main = "Gamma = 3")
plot_performance_profile(all_approaches[all_approaches$gamma == 4,], main = "Gamma = 4")
```