---
title: Open methodology for "Exact approach for convex adjustable robust optimization" > Facility Location Problem (FLP)
output: 
  html_document:
    theme: null
    css: /assets/css/design.css
    self_contained: false
    highlight: null
    include:
      in_header: ../../../_includes/head.html
      before_body: 
        - ../../../_includes/header.html
        - ../../../_includes/begin_content.html
        - ../../../_includes/toc.html
        - ../../../_includes/begin_post_content.html
      after_body: 
        - ../../../_includes/handle_page_title.html
        - ../../../_includes/end_post_content.html
        - ../../../_includes/end_content.html
        - ../../../_includes/footer.html
---

<div class="warning">This document is automatically generated after every `git push` action on the public repository `hlefebvr/hlefebvr.github.io` using rmarkdown and Github Actions. This ensures total reproducibility of our data manipulation. Last compilation: `r format(Sys.time(), '%d/%m/%y %H:%M:%S')`.</div>


```{r echo=FALSE}
library(rmarkdown)
library(kableExtra)
library(tidyr)
library(ggplot2)
suppressPackageStartupMessages(library(dplyr))
```

## Loading the data

The raw results can be found in the file `results.csv` with the following columns:

- tag: a tag which should always equal "result" used to `grep` the result line in the execution log file.
- instance: the name or path of the instance.
- n_facilities: the number of facilities in the instance.
- n_customers: the number of customers in the instance.
- Gamma: the value of $\Gamma$ which was used.
- deviation: the maximum deviation, in percentage, from the nominal demand.
- time_limit: the time limit which was used when solving the instance.
- method: the name of the method which was used to solve the instance.
- total_time: the total time used to solve the instance.
- master_time: the time spent solving the master problem.
- separation_time: the time spent solving the separation problem.
- best_bound: the best bound found.
- iteration_count: the number of iterations.
- fail: should be empty if the execution of the algorithm went well.

We start by reading the file and by removing the "tag" column.

```{r}
results = read.table("./results.csv", header = FALSE, sep = ',')
colnames(results) <- c("tag", "instance", "n_facilities", "n_customers", "Gamma", "deviation", "time_limit", "method", "total_time", "master_time", "separation_time", "best_bound", "iteration_count", "fail")
results$tag = NULL

results = results[!(results$n_facilities == 10 & results$n_customers == 30),]
results = results[!(results$n_facilities == 15 & results$n_customers == 40),]
```

We then check that all instances were solved without issue by checking the "fail" column.

```{r}
sum(results$fail)
```

Then, we compute the percentage which $\Gamma$ represents for the total number of customers

```{r}
results$p = results$Gamma / results$n_customers
results$p = ceiling(results$p / .05) * .05
```

```{r}
results = results %>%
  mutate(
    ratio_demand_capacity = as.integer(sub('.*instance_F\\d+_C\\d+_R(\\d+)__\\d+\\.txt', '\\1', instance))
  )

results$size = paste0("(", results$n_facilities, ",", results$n_customers, "), ", results$p)
```

We add a tag for unsolved instances.

```{r}
time_limit = 7200

results$unsolved = results$total_time >= time_limit | results$fail
```

All in all, our result data reads.

```{r, echo = FALSE}
paged_table(results)
```


Additionally, we define the variable `time_limit` which stores the time limit used in our computation.

```{r}
time_limit = 7200
```

## Unsolved instances

```{r}
for (method in unique(results$method)) {
  
  # Sum of unsolved cases for each size
  sum_unsolved = results[results$method == method,] %>% group_by(size) %>% summarise(total_unsolved = sum(unsolved))
  
  # Create a bar plot
  p = ggplot(sum_unsolved, aes(x = size, y = total_unsolved)) +
    geom_bar(stat = "identity") +
    labs(x = "Size", y = "Total Unsolved Cases", title = method) +
    theme_minimal()

  print(p)
}
```

## Performance profiles and ECDF

We now introduce a function which plots the performance profile of our solvers over a given test set.

```{r}
add_performance_ratio = function(dataset, 
                                 criterion_column = "total_time",
                                 unsolved_column = "unsolved",
                                 instance_column = "instance",
                                 solver_column = "solver",
                                 output_column = "performance_ratio") {
  
  # Compute best score for each instance
  best = dataset %>%
    group_by(!!sym(instance_column)) %>%
    mutate(best_solver = min(!!sym(criterion_column)))
  
  # Compute performance ratio for each instance and solver
  result = best %>%
    group_by(!!sym(instance_column), !!sym(solver_column)) %>%
    mutate(!!sym(output_column) := !!sym(criterion_column) / best_solver) %>%
    ungroup()
  
  if (sum(result[,unsolved_column]) > 0) {
    result[result[,unsolved_column] == TRUE,output_column] = max(result[,output_column])
  }

  return (result)
}

plot_performance_profile = function(dataset,
                                    criterion_column,
                                    unsolved_column = "unsolved",
                                    instance_column = "instance",
                                    solver_column = "solver"
                                    ) {
  
  dataset_with_performance_ratios = add_performance_ratio(dataset,
                                                          criterion_column = criterion_column,
                                                          instance_column = instance_column,
                                                          solver_column = solver_column,
                                                          unsolved_column = unsolved_column)
  
  solved_dataset_with_performance_ratios = dataset_with_performance_ratios[!dataset_with_performance_ratios[,unsolved_column],]
  
  compute_performance_profile_point = function(method, data) {
    
    performance_ratios = solved_dataset_with_performance_ratios[solved_dataset_with_performance_ratios[,solver_column] == method,]$performance_ratio
    
    unscaled_performance_profile_point = ecdf(performance_ratios)(data)
    
    n_instances = sum(dataset[,solver_column] == method)
    n_solved_instances = sum(dataset[,solver_column] == method & !dataset[,unsolved_column])
    
    return( unscaled_performance_profile_point * n_solved_instances / n_instances )
  }
  
  perf = solved_dataset_with_performance_ratios %>%
    group_by(!!sym(solver_column)) %>%
    mutate(performance_profile_point = compute_performance_profile_point(unique(!!sym(solver_column)), performance_ratio))
  
  result = ggplot(data = perf, aes(x = performance_ratio, y = performance_profile_point, color = !!sym(solver_column))) +
              geom_line()
  
  return (result)
}

```



```{r}
plot_performance_profile(results, criterion_column = "total_time", solver_column = "method") +
  labs(x = "Performance ratio", y = "% of instances") +
  scale_y_continuous(limits = c(0, 1)) +
  theme_minimal()
```


```{r}
for (ratio in unique(results$ratio_demand_capacity)) {
  
  p = plot_performance_profile(results[results$ratio_demand_capacity == ratio,], criterion_column = "total_time", solver_column = "method") +
    labs(x = "Performance ratio", y = "% of instances", title = paste0("R = ", ratio / 1000)) +
    scale_y_continuous(limits = c(0, 1)) +
    theme_minimal()
  
  print(p)
}
```

As a complement, we also draw the ECDF.

```{r}
ggplot(results, aes(x = total_time, color = method)) +
  stat_ecdf(geom = "step") +
  xlab("Total Time") +
  ylab("ECDF") +
  labs(title = "ECDF of Total Time by Method")  +
  scale_x_continuous(breaks = seq(0, max(results$total_time), by = 500), labels = seq(0, max(results$total_time), by = 500)) +
  theme_minimal()
```


### Summary table

In this section, we create a table summarizing the outcome of our experiments.

We start by computing average computation times over the solved instances.

```{r}
results_solved = results %>%
  filter(total_time < 7200) %>%
  group_by(method, n_facilities, n_customers, p, deviation) %>%
  summarize(
    solved = n(),
    total_time = mean(total_time),
    master_time = mean(master_time),
    separation_time = mean(separation_time),
    solved_iteration_count = mean(iteration_count),
    .groups = "drop"
  ) %>%
  ungroup()
```

Then, we also consider unsolved instances: we count the number of such instances and compute the average iteration count.

```{r}
results_unsolved <- results %>%
  filter(total_time >= 7200) %>%
  group_by(method, n_facilities, n_customers, p, deviation) %>%
  summarize(
    unsolved = n(),
    unsolved_iteration_count = mean(iteration_count),
    .groups = "drop"
  ) %>%
  ungroup()
```


We then merge the two tables.

```{r}
final_result <- merge(results_solved, results_unsolved, by = c("method", "n_facilities", "n_customers", "p", "deviation"), all = TRUE)
```

Finally, we replace all `NA` entries by 0.

```{r}
final_result[is.na(final_result)] = 0
```

Here is our table.

```{r, echo = FALSE}
knitr::kable(final_result, 
               digits = c(0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2), 
               col.names = c("Method", "|V_1|", "|V_2|", "p", "dev", "Count", "Total", "Master", "Sepatation", "# Iter", "Count", "# Iter"),
               caption = "Summary table"
             ) %>%
      kable_classic() %>%
      add_header_above(c(" " = 5, "Solved instances" = 5, "Unsolved instances" = 2)
)
```

## Number of iterations

```{r}
for (method in unique(results$method)) {
  
  p = ggplot(results[results$method == method & results$total_time < time_limit,], aes(x = iteration_count)) +
    geom_bar() +
    labs(
      title = method,
      x = "Number of iterations",
      y = "Number of instances"
    ) +
    theme_minimal()
    
  print(p) 
}
```


